üèóÔ∏è Fase 1: Infraestructura y Pipeline de Ingesti√≥n (Data Engineering)

En la etapa inicial del proyecto, se estableci√≥ una base s√≥lida de ingenier√≠a de datos enfocada en la reproducibilidad, escalabilidad y buenas pr√°cticas de dise√±o de software.

üîß Stack Tecnol√≥gico & Infraestructura

Gesti√≥n de Dependencias: Uso de uv (Astral) para un entorno virtual determinista y ultrarr√°pido, reemplazando herramientas tradicionales para optimizar tiempos de CI/CD.

Vector Database: Despliegue de Qdrant mediante Docker Compose, asegurando persistencia de datos (Volumes) y aislamiento del entorno.

Control de Versiones: Estrategia de Git ignorando artefactos pesados y secretos (.env), siguiendo flujos de trabajo est√°ndar.

‚öôÔ∏è Pipeline de Extracci√≥n (ETL)

Se implement√≥ un m√≥dulo de ingesti√≥n de datos siguiendo estrictamente los principios SOLID para garantizar mantenibilidad:

Abstracci√≥n (Interfaces): Definici√≥n de contratos BaseLoader y BaseCleaner (Open/Closed Principle), permitiendo extender el sistema a nuevos formatos (CSV, SQL) sin modificar el c√≥digo base.

Extracci√≥n Optimizada: Implementaci√≥n de PDFLoader utilizando librer√≠as ligeras (pypdf) para reducir el Cold Start y el consumo de memoria, evitando la sobrecarga de dependencias innecesarias.

Limpieza Modular: Estrategia de limpieza desacoplada (MedicalTextCleaner) inyectada como dependencia (Dependency Injection), facilitando pruebas unitarias y cambios en la l√≥gica de preprocesamiento.

üß† Fase 2: Transformaci√≥n, Vectorizaci√≥n y Estrategia de Recuperaci√≥n

En esta fase se transform√≥ la data cruda en una estructura optimizada para RAG, priorizando la precisi√≥n sem√°ntica sin sacrificar la riqueza del contexto necesario para el LLM.

üß© Estrategia de Splitting (Parent-Document Pattern)

Se implement√≥ una arquitectura de datos jer√°rquica para resolver el compromiso entre precisi√≥n de b√∫squeda y ventana de contexto:

Desacoplamiento Contexto/√çndice: Generaci√≥n de Child Chunks peque√±os (optimizados para similitud coseno) vinculados a Parent Chunks grandes (optimizados para comprensi√≥n del LLM).

Trazabilidad Relacional: Vinculaci√≥n mediante UUIDs y metadatos (parent_id) para asegurar la integridad referencial entre √≠ndices de b√∫squeda y almacenamiento de contenido.

üíæ Vectorizaci√≥n y Almacenamiento (Batching)

Embeddings Locales: Integraci√≥n de sentence-transformers (all-MiniLM-L6-v2) para inferencia local r√°pida, eliminando costes de API para la vectorizaci√≥n.

Ingesti√≥n por Lotes: Implementaci√≥n de carga masiva (batch_size=64) en Qdrant para minimizar la latencia de red y optimizar el throughput de escritura (I/O).

Idempotencia: L√≥gica de Upsert basada en IDs deterministas para permitir re-ejecuciones del pipeline sin generar duplicados.

üîç Recuperaci√≥n Avanzada (Retrieval)

Query Optimization: Uso de Filter Push-down en Qdrant para restringir la b√∫squeda vectorial estrictamente a los fragmentos "hijos".

Reconstrucci√≥n de Contexto: L√≥gica de recuperaci√≥n en dos pasos: B√∫squeda Vectorial Aproximada (ANN) $\to$ Recuperaci√≥n de Puntos por ID (Lookup O(1)) para entregar el documento padre completo al modelo generativo.

üß† Fase 3: Orquestaci√≥n Inteligente y Optimizaci√≥n (RAG Avanzado)

Se evolucion√≥ el sistema de un simple buscador a un asistente conversacional con memoria y capacidad de razonamiento refinada.

ü§ñ Generaci√≥n y Memoria (LLM Integration)

Integraci√≥n de Gemini 1.5: Implementaci√≥n del modelo gemini-1.5-flash v√≠a langchain-google-genai para la s√≠ntesis de respuestas, aprovechando su baja latencia y amplia ventana de contexto.

Gesti√≥n de Historial (Conversational Memory): Desarrollo de un sistema de memoria de ventana deslizante ("Sliding Window") manual.

Query Rewriting: Implementaci√≥n de un paso intermedio donde el LLM reformula la pregunta del usuario bas√°ndose en el historial del chat (ej. transformar "¬øY cu√°les son sus riesgos?" a "¬øCu√°les son los riesgos del iDML?") antes de consultar la base vectorial.

‚öñÔ∏è Reranking (Precisi√≥n Sem√°ntica)

Se a√±adi√≥ una segunda etapa de filtrado para resolver las limitaciones de la b√∫squeda por similitud coseno (Bi-Encoders):

Arquitectura Two-Stage Retrieval:

Wide Fetch: Qdrant recupera ~20 candidatos bas√°ndose en similitud vectorial aproximada.

Deep Rerank: FlashRank (Cross-Encoder ligero corriendo en CPU) reordena los candidatos analizando la interacci√≥n profunda entre la pregunta y cada documento.

Resultado: Mejora dr√°stica en la relevancia de los documentos enviados al LLM, descartando "falsos positivos" que se parecen vectorialmente pero no sem√°nticamente.

üõ°Ô∏è Robustez y Patrones de Dise√±o

Refactorizaci√≥n SOLID: Aplicaci√≥n de Dependency Injection en el constructor del chatbot para desacoplar el servicio de recuperaci√≥n.

Manejo de Fallos (Graceful Degradation): Implementaci√≥n de bloques try-except personalizados y excepciones VectorDBConnectionError para garantizar que el bot informe problemas de infraestructura amigablemente en lugar de colapsar.