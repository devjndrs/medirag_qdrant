# ü©∫ MediRAG: Advanced Medical RAG System

![Python](https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python&logoColor=white)
![Qdrant](https://img.shields.io/badge/Qdrant-Vector_DB-red?style=for-the-badge&logo=qdrant&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-Framework-green?style=for-the-badge&logo=langchain&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Containerization-2496ED?style=for-the-badge&logo=docker&logoColor=white)

## üìñ Introducci√≥n

**MediRAG** es un sistema de **Generaci√≥n Aumentada por Recuperaci√≥n (RAG)** de grado industrial dise√±ado para el dominio m√©dico. A diferencia de los tutoriales b√°sicos de RAG, este proyecto se centra en la **ingenier√≠a de datos robusta**, la arquitectura modular y los patrones de dise√±o avanzados necesarios para desplegar sistemas de IA generativa en producci√≥n.

Este repositorio no es solo un chatbot; es una implementaci√≥n de referencia de c√≥mo construir pipelines de datos resilientes, escalables y mantenibles para aplicaciones de LLM.

---

## üéØ Importancia para Equipos de ML e IA

En el ecosistema actual de IA Generativa, el **80% del √©xito de un sistema RAG reside en la calidad de su ingenier√≠a de datos**, no solo en el modelo de lenguaje elegido.

Para un equipo de ML/IA, adoptar un enfoque de ingenier√≠a como el de MediRAG ofrece ventajas cr√≠ticas:

### üöÄ Ventajas Competitivas
1.  **Reproducibilidad y Determinismo**: El uso de entornos gestionados (`uv`, Docker) y pipelines orquestados elimina el problema de "funciona en mi m√°quina".
2.  **Calidad de Datos Superior**: La implementaci√≥n de estrategias de *Parent-Child Splitting* y *Reranking* asegura que el LLM reciba contexto preciso, reduciendo dr√°sticamente las alucinaciones.
3.  **Mantenibilidad a Largo Plazo**: La arquitectura basada en principios **SOLID** y **Dependency Injection** permite cambiar componentes (ej. cambiar Qdrant por Pinecone, o Gemini por GPT-4) sin reescribir el n√∫cleo del sistema.
4.  **Observabilidad y Testing**: Tratar los datos como c√≥digo mediante tests de integridad y pipelines de validaci√≥n E2E permite detectar degradaci√≥n en la calidad de las respuestas antes de llegar a producci√≥n.

---

## üèóÔ∏è Arquitectura y Fases del Proyecto

### üîπ Fase 1: Infraestructura y Pipeline de Ingesti√≥n (Data Engineering)
Establecimiento de una base s√≥lida enfocada en la reproducibilidad y escalabilidad.

*   **Gesti√≥n de Dependencias**: Uso de `uv` para entornos virtuales deterministas.
*   **Vector Database**: Despliegue de **Qdrant** v√≠a Docker Compose con persistencia de datos.
*   **ETL Modular (SOLID)**:
    *   **Abstracci√≥n**: Interfaces `BaseLoader` y `BaseCleaner` para extensibilidad.
    *   **Extracci√≥n**: `PDFLoader` optimizado con `pypdf`.
    *   **Limpieza**: `MedicalTextCleaner` inyectado como dependencia para facilitar tests.

### üîπ Fase 2: Transformaci√≥n y Estrategia de Recuperaci√≥n
Transformaci√≥n de data cruda en estructuras optimizadas para b√∫squeda sem√°ntica.

*   **Parent-Document Pattern**:
    *   *Child Chunks*: Peque√±os, optimizados para b√∫squeda vectorial (similitud coseno).
    *   *Parent Chunks*: Grandes, optimizados para dar contexto completo al LLM.
*   **Vectorizaci√≥n Local**: Uso de `sentence-transformers` para inferencia r√°pida y sin coste.
*   **Ingesti√≥n por Lotes**: Carga masiva en Qdrant para optimizar I/O de red.

### üîπ Fase 3: Orquestaci√≥n Inteligente (Advanced RAG)
Evoluci√≥n hacia un asistente conversacional con razonamiento refinado.

*   **LLM Integration**: Google Gemini 1.5 Flash para s√≠ntesis de respuestas.
*   **Memoria Conversacional**: Sistema de ventana deslizante para mantener contexto del chat.
*   **Query Rewriting**: Reformulaci√≥n de preguntas basada en el historial para mejorar el retrieval.
*   **Two-Stage Retrieval**:
    1.  **Wide Fetch**: B√∫squeda vectorial r√°pida (High Recall).
    2.  **Deep Rerank**: Reordenamiento con **FlashRank** (Cross-Encoder) para m√°xima precisi√≥n sem√°ntica.

### üîπ Fase 4: Validaci√≥n y CI/CD de Datos
Garant√≠a de fiabilidad en entornos productivos.

*   **Pipeline de Pruebas E2E**: Orquestador (`src/run_pipeline.py`) que valida secuencialmente:
    1.  Sanity Checks (Entorno/DB).
    2.  Integridad de Datos (ETL).
    3.  L√≥gica de Transformaci√≥n.
    4.  Calidad de Retrieval y Reranking.
    5.  Generaci√≥n Final.

---

## üõ†Ô∏è Gu√≠a de Instalaci√≥n y Uso

### Prerrequisitos
*   **Docker** y **Docker Compose** instalados.
*   **Python 3.11+**.
*   **uv** (Recomendado) o `pip`.
*   API Key de Google Gemini (en `.env`).

### 1. Configuraci√≥n del Entorno

```bash
# Clonar el repositorio
git clone <repo-url>
cd chatbotMedico

# Crear entorno virtual e instalar dependencias
uv venv
source .venv/bin/activate
uv pip install -r pyproject.toml  # O requirements.txt si se genera
```

### 2. Levantar Infraestructura

```bash
# Iniciar Qdrant
docker-compose up -d
```

### 3. Ejecutar Pipeline Completo (Validaci√≥n E2E)

Para ejecutar todo el flujo, desde la ingesta hasta la prueba del chat, utiliza el orquestador:

```bash
python src/run_pipeline.py
```

Este comando ejecutar√° autom√°ticamente:
*   Verificaci√≥n de conexi√≥n a Qdrant.
*   Descarga del paper m√©dico de muestra.
*   Procesamiento, limpieza y vectorizaci√≥n.
*   Pruebas de b√∫squeda y generaci√≥n de respuesta.

---

## üìÇ Estructura del Proyecto

```text
src/
‚îú‚îÄ‚îÄ core/           # Configuraci√≥n y definiciones de tipos
‚îú‚îÄ‚îÄ ingestion/      # Loaders, Cleaners y Splitters (ETL)
‚îú‚îÄ‚îÄ retrieval/      # L√≥gica de b√∫squeda y Reranking
‚îú‚îÄ‚îÄ generation/     # Integraci√≥n con LLM y cadenas RAG
‚îú‚îÄ‚îÄ vector_store/   # Cliente y gesti√≥n de Qdrant
‚îú‚îÄ‚îÄ testing/        # Tests unitarios y de integraci√≥n
‚îî‚îÄ‚îÄ run_pipeline.py # Orquestador maestro
```

---

> **Nota**: Este proyecto demuestra que un sistema RAG efectivo es mucho m√°s que un script de 50 l√≠neas. Es un sistema de ingenier√≠a de software completo que requiere dise√±o, pruebas y una arquitectura s√≥lida.

üß† Fase 2: Transformaci√≥n, Vectorizaci√≥n y Estrategia de Recuperaci√≥n

En esta fase se transform√≥ la data cruda en una estructura optimizada para RAG, priorizando la precisi√≥n sem√°ntica sin sacrificar la riqueza del contexto necesario para el LLM.

üß© Estrategia de Splitting (Parent-Document Pattern)

Se implement√≥ una arquitectura de datos jer√°rquica para resolver el compromiso entre precisi√≥n de b√∫squeda y ventana de contexto:

Desacoplamiento Contexto/√çndice: Generaci√≥n de Child Chunks peque√±os (optimizados para similitud coseno) vinculados a Parent Chunks grandes (optimizados para comprensi√≥n del LLM).

Trazabilidad Relacional: Vinculaci√≥n mediante UUIDs y metadatos (parent_id) para asegurar la integridad referencial entre √≠ndices de b√∫squeda y almacenamiento de contenido.

üíæ Vectorizaci√≥n y Almacenamiento (Batching)

Embeddings Locales: Integraci√≥n de sentence-transformers (all-MiniLM-L6-v2) para inferencia local r√°pida, eliminando costes de API para la vectorizaci√≥n.

Ingesti√≥n por Lotes: Implementaci√≥n de carga masiva (batch_size=64) en Qdrant para minimizar la latencia de red y optimizar el throughput de escritura (I/O).

Idempotencia: L√≥gica de Upsert basada en IDs deterministas para permitir re-ejecuciones del pipeline sin generar duplicados.

üîç Recuperaci√≥n Avanzada (Retrieval)

Query Optimization: Uso de Filter Push-down en Qdrant para restringir la b√∫squeda vectorial estrictamente a los fragmentos "hijos".

Reconstrucci√≥n de Contexto: L√≥gica de recuperaci√≥n en dos pasos: B√∫squeda Vectorial Aproximada (ANN) $\to$ Recuperaci√≥n de Puntos por ID (Lookup O(1)) para entregar el documento padre completo al modelo generativo.

üß† Fase 3: Orquestaci√≥n Inteligente y Optimizaci√≥n (RAG Avanzado)

Se evolucion√≥ el sistema de un simple buscador a un asistente conversacional con memoria y capacidad de razonamiento refinada.

ü§ñ Generaci√≥n y Memoria (LLM Integration)

Integraci√≥n de Gemini 1.5: Implementaci√≥n del modelo gemini-1.5-flash v√≠a langchain-google-genai para la s√≠ntesis de respuestas, aprovechando su baja latencia y amplia ventana de contexto.

Gesti√≥n de Historial (Conversational Memory): Desarrollo de un sistema de memoria de ventana deslizante ("Sliding Window") manual.

Query Rewriting: Implementaci√≥n de un paso intermedio donde el LLM reformula la pregunta del usuario bas√°ndose en el historial del chat (ej. transformar "¬øY cu√°les son sus riesgos?" a "¬øCu√°les son los riesgos del iDML?") antes de consultar la base vectorial.

‚öñÔ∏è Reranking (Precisi√≥n Sem√°ntica)

Se a√±adi√≥ una segunda etapa de filtrado para resolver las limitaciones de la b√∫squeda por similitud coseno (Bi-Encoders):

Arquitectura Two-Stage Retrieval:

Wide Fetch: Qdrant recupera ~20 candidatos bas√°ndose en similitud vectorial aproximada.

Deep Rerank: FlashRank (Cross-Encoder ligero corriendo en CPU) reordena los candidatos analizando la interacci√≥n profunda entre la pregunta y cada documento.

Resultado: Mejora dr√°stica en la relevancia de los documentos enviados al LLM, descartando "falsos positivos" que se parecen vectorialmente pero no sem√°nticamente.

üõ°Ô∏è Robustez y Patrones de Dise√±o

Refactorizaci√≥n SOLID: Aplicaci√≥n de Dependency Injection en el constructor del chatbot para desacoplar el servicio de recuperaci√≥n.

Manejo de Fallos (Graceful Degradation): Implementaci√≥n de bloques try-except personalizados y excepciones VectorDBConnectionError para garantizar que el bot informe problemas de infraestructura amigablemente en lugar de colapsar.