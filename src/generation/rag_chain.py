from typing import List, Tuple
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from core.config import settings
from retrieval.services import MedicalRetriever, VectorDBConnectionError

class MedicalChatBot:
    """
    Clase principal que orquesta el flujo RAG usando Google Gemini.
    """
    
    def __init__(self, retriever_service: MedicalRetriever):
        # 1. Inyecci√≥n de Dependencia (DIP)
        self.retriever_service = retriever_service
        
        if not settings.GOOGLE_API_KEY:
            raise ValueError("‚ö†Ô∏è GOOGLE_API_KEY falta en .env")
            
        self.llm = ChatGoogleGenerativeAI(
            model=settings.LLM_MODEL_NAME,
            temperature=settings.TEMPERATURE,
            google_api_key=settings.GOOGLE_API_KEY
        )
        
        # 1. Prompt para Contextualizar (Query Rewriting)
        # Este prompt reformula la pregunta del usuario usando el historial
        # para que tenga sentido por s√≠ sola (standalone question).
        self.contextualize_q_system_prompt = """Dada una historia de chat y la √∫ltima pregunta del usuario 
        (que podr√≠a hacer referencia al contexto anterior), formula una pregunta independiente 
        que pueda entenderse sin el historial. NO respondas la pregunta, solo reform√∫lala si es necesario 
        o devu√©lvela tal cual si ya es explicita."""
        
        self.contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("system", self.contextualize_q_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ])
        
        self.history_chain = self.contextualize_q_prompt | self.llm | StrOutputParser()

        # 2. Prompt Principal (QA)
        self.qa_system_prompt = """Eres un asistente m√©dico experto. Usa los siguientes fragmentos de contexto recuperado para responder la pregunta.
        Si no sabes la respuesta, di que no lo sabes. Usa un m√°ximo de tres oraciones y s√© conciso.
        
        Contexto:
        {context}
        """
        
        self.qa_prompt = ChatPromptTemplate.from_messages([
            ("system", self.qa_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"), # Incluimos historia para mantener tono
            ("human", "{question}"),
        ])

    def _format_docs(self, docs) -> str:
        formatted = []
        for i, doc in enumerate(docs):
            content = doc.page_content.replace("\n", " ")
            meta = doc.metadata
            formatted.append(f"[Fuente: {meta.get('source')} (P√°g {meta.get('page')})]: {content}")
        return "\n\n".join(formatted)

    def answer(self, query: str, chat_history: List[Tuple[str, str]] = []):
        """
        Args:
            query: Pregunta actual.
            chat_history: Lista de tuplas (Usuario, IA) con la conversaci√≥n previa.
        """
        # Convertir historial de tuplas a formato LangChain
        lc_history = []
        for human, ai in chat_history:
            lc_history.append(HumanMessage(content=human))
            lc_history.append(AIMessage(content=ai))

        print(f"ü§î Pregunta original: {query}")

        # 1. Reformular pregunta si hay historial
        if lc_history:
            refined_query = self.history_chain.invoke({
                "chat_history": lc_history,
                "question": query
            })
            print(f"üîÑ Pregunta reescrita (contextualizada): {refined_query}")
        else:
            refined_query = query

        # 2. Recuperar documentos usando la pregunta REFINADA
        try:
            relevant_docs = self.retriever_service.search(refined_query, k=4)
        except VectorDBConnectionError as e:
            print(f"‚ö†Ô∏è Fallo en recuperaci√≥n: {e}")
            return "‚ö†Ô∏è Error: No puedo acceder a mi memoria m√©dica en este momento. Por favor verifica que el servicio de Qdrant est√© activo.", []
        
        if not relevant_docs:
            return "No encontr√© informaci√≥n relevante.", []

        # 3. Generar respuesta final
        context_str = self._format_docs(relevant_docs)
        
        qa_chain = self.qa_prompt | self.llm | StrOutputParser()
        
        response = qa_chain.invoke({
            "chat_history": lc_history,
            "context": context_str,
            "question": refined_query # Pasamos la refinada al LLM para que enfoque su respuesta
        })
        
        return response, relevant_docs